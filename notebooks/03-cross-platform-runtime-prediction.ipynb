{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../bin')\n",
    "\n",
    "from IPython.display import display\n",
    "import json\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import \\\n",
    "    mean_absolute_error\n",
    "\n",
    "from utils import \\\n",
    "    UNITS, \\\n",
    "    anomaly_score, \\\n",
    "    check_std, \\\n",
    "    predict_intervals\n",
    "\n",
    "from train import \\\n",
    "    load_dataset, \\\n",
    "    load_datasets, \\\n",
    "    is_categorical, \\\n",
    "    create_dataset, \\\n",
    "    create_mlp, \\\n",
    "    create_rf, \\\n",
    "    create_pipeline, \\\n",
    "    mean_absolute_percentage_error, \\\n",
    "    prediction_interval_coverage, \\\n",
    "    coverage_error, \\\n",
    "    evaluate_cv\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Minibench Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'node_type',\n",
    "    'cpu_iops',\n",
    "    'cpu_flops',\n",
    "    'cpu_mem_bw',\n",
    "    'disk_read',\n",
    "    'disk_write',\n",
    "    'gpu_flops',\n",
    "    'gpu_mem_bw'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def plot_minibench(df, outfile, w=16, h=4):\n",
    "    # sort data by node type\n",
    "    df = df.sort_values('node_type')\n",
    "\n",
    "    # rename metrics to include units\n",
    "    mapper = {\n",
    "        'cpu_flops':  'cpu_flops (GFLOP/s)',\n",
    "        'cpu_mem_bw': 'cpu_mem_bw (GB/s)',\n",
    "        'disk_read':  'disk_read (GB/s)',\n",
    "        'disk_write': 'disk_write (GB/s)',\n",
    "        'gpu_flops':  'gpu_flops (GFLOP/s)',\n",
    "        'gpu_mem_bw': 'gpu_mem_bw (GB/s)',\n",
    "    }\n",
    "\n",
    "    df = df.rename(columns=mapper)\n",
    "\n",
    "    # plot minibench metrics\n",
    "    plots = [\n",
    "        ('node_type', mapper['cpu_flops']),\n",
    "        ('node_type', mapper['cpu_mem_bw']),\n",
    "        ('node_type', mapper['disk_read']),\n",
    "        ('node_type', mapper['disk_write']),\n",
    "        ('node_type', mapper['gpu_flops']),\n",
    "        ('node_type', mapper['gpu_mem_bw']),\n",
    "    ]\n",
    "\n",
    "    plt.subplots(len(plots), 1, figsize=(w, h * len(plots)), sharex=True)\n",
    "\n",
    "    for i, (x, y) in enumerate(plots):\n",
    "        plt.subplot(len(plots), 1, i + 1)\n",
    "\n",
    "        if x != None:\n",
    "            sns.barplot(x=x, y=y, data=df, palette='rocket', ci=68)\n",
    "        else:\n",
    "            sns.histplot(y=y, data=df)\n",
    "\n",
    "        # plt.xticks(rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palmetto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load palmetto data\n",
    "df = load_dataset('../../minibench/trace-palmetto/minibench.minibench.trace.txt')\n",
    "df = df[columns]\n",
    "\n",
    "# fix issues with gpu metrics\n",
    "df.loc[df['gpu_flops'].abs() > 1e7, 'gpu_flops'] = 0\n",
    "df.loc[df['gpu_mem_bw'].abs() > 1e4, 'gpu_mem_bw'] = 0\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_sort(x):\n",
    "    n = int(re.search(r'\\d+', x)[0])\n",
    "    p = x[-1] if x[-1].isalpha() else ''\n",
    "    \n",
    "    return '%02d%s' % (n, p)\n",
    "\n",
    "# format node_type values to be in order\n",
    "df['node_type'] = df['node_type'].apply(lambda x: version_sort(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_minibench(df, '03-minibench-palmetto.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nautilus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nautilus data\n",
    "df = load_dataset('../../minibench/trace-nautilus/minibench.minibench.trace.txt')\n",
    "df = df[columns]\n",
    "\n",
    "# fix issues with gpu metrics\n",
    "df['gpu_flops'] = df['gpu_flops'].astype(np.float64)\n",
    "df['gpu_mem_bw'] = df['gpu_mem_bw'].astype(np.float64)\n",
    "\n",
    "df.loc[df['gpu_flops'].abs() > 1e7, 'gpu_flops'] = 0\n",
    "df.loc[df['gpu_mem_bw'].abs() > 1e4, 'gpu_mem_bw'] = 0\n",
    "\n",
    "# rename node types for plot\n",
    "mapping = {\n",
    "    '1060': 'GTX 1060',\n",
    "    '1070': 'GTX 1070',\n",
    "    '1080': 'GTX 1080',\n",
    "    '1080Ti': 'GTX 1080Ti',\n",
    "    '2080Ti': 'RTX 2080Ti',\n",
    "    '3090': 'RTX 3090',\n",
    "    'A100': 'Tesla A100',\n",
    "    'A40': 'Tesla A40',\n",
    "    'M4000': 'Quadro M4000',\n",
    "    'T4': 'Tesla T4',\n",
    "    'TITANRTX': 'TITAN RTX',\n",
    "    'V100': 'Tesla V100',\n",
    "    'cpu': 'CPU',\n",
    "    'titan-xp': 'TITAN Xp',\n",
    "}\n",
    "\n",
    "for k, v in mapping.items():\n",
    "    df['node_type'] = df['node_type'].str.replace('^' + k + '$', v, regex=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nautilus node types\n",
    "df_nodes = pd.read_fwf('../../minibench/trace-nautilus/nautilus-node-types.txt')\n",
    "\n",
    "mapper = {\n",
    "    'NAME': 'hostname',\n",
    "    'PROCESSOR': 'processor',\n",
    "    'GPU-TYPE': 'gpu_type'\n",
    "}\n",
    "\n",
    "df_nodes = df_nodes.rename(columns=mapper)\n",
    "df_nodes = df_nodes[mapper.values()]\n",
    "df_nodes = df_nodes.fillna('')\n",
    "\n",
    "df_nodes['gpu_type'] = df_nodes['gpu_type'].str.replace('^$', 'cpu', regex=True)\n",
    "\n",
    "display(df_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_minibench(df, '03-minibench-nautilus.pdf', w=12, h=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load google cloud data\n",
    "df = load_dataset('../../minibench/trace-google/minibench.minibench.trace.txt')\n",
    "df = df[columns]\n",
    "\n",
    "# fix issues with gpu metrics\n",
    "df['gpu_flops'] = df['gpu_flops'].astype(np.float64)\n",
    "df['gpu_mem_bw'] = df['gpu_mem_bw'].astype(np.float64)\n",
    "\n",
    "df.loc[df['gpu_flops'].abs() > 1e7, 'gpu_flops'] = 0\n",
    "df.loc[df['gpu_mem_bw'].abs() > 1e4, 'gpu_mem_bw'] = 0\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_minibench(df, '03-minibench-google.pdf', w=4, h=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trace Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "config = json.load(open('../workflows/kinc/params.json'))\n",
    "\n",
    "# load trace data\n",
    "dataset_names = [\n",
    "    'breast-palmetto',\n",
    "    'breast-nautilus',\n",
    "    'unified-nautilus'\n",
    "]\n",
    "data_map = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    base_dir = '../_datasets/kinc-%s' % (dataset_name)\n",
    "\n",
    "    # aggregate minibench data before merging\n",
    "    df = load_dataset('%s/minibench.minibench.trace.txt' % (base_dir))\n",
    "    df = df.drop(columns=['hash', 'cpus', 'time', 'disk', 'memory'], errors='ignore')\n",
    "    df['gpu_flops'] = df['gpu_flops'].astype(np.float64)\n",
    "    df['gpu_mem_bw'] = df['gpu_mem_bw'].astype(np.float64)\n",
    "    df.loc[df['gpu_flops'].abs() > 1e7, 'gpu_flops'] = 0\n",
    "    df.loc[df['gpu_mem_bw'].abs() > 1e4, 'gpu_mem_bw'] = 0\n",
    "    df = df.groupby('node_type').mean()\n",
    "    df.to_csv('%s/minibench.minibench.trace.txt' % (base_dir), sep='\\t')\n",
    "\n",
    "    process_names = config['train_inputs'].keys()\n",
    "    merge_files = [arg.split(' ') for arg in config['train_merge_args']]\n",
    "\n",
    "    data_map[dataset_name] = load_datasets('kinc', process_names, base_dir=base_dir, merge_files=merge_files)\n",
    "\n",
    "# remove additional unused columns\n",
    "for dataset_name in data_map.keys():\n",
    "    for process_name, df in data_map[dataset_name].items():\n",
    "        df['platform'] = dataset_name.split('-')[1]\n",
    "        df = df.drop(columns=['hash', 'cpus', 'time', 'disk', 'memory', 'workdir', 'hostname'], errors='ignore')\n",
    "        data_map[dataset_name][process_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one similarity_chunk run for each unique workflow run\n",
    "for dataset_name in ['breast-palmetto']:\n",
    "    df = data_map[dataset_name]['similarity_chunk']\n",
    "\n",
    "    df = resample(df, ['dataset', 'n_rows', 'n_cols', 'hardware_type', 'chunks', 'threads', 'platform', 'node_type'])\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    data_map[dataset_name]['similarity_chunk'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Platform Runtime Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palmetto P100/V100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_name, target = 'similarity_chunk', 'runtime_hr'\n",
    "\n",
    "# get performance data for pipeline / process\n",
    "df = data_map['breast-palmetto'][process_name]\n",
    "inputs = config['train_inputs'][process_name]\n",
    "\n",
    "# remove samples with missing data\n",
    "df = df.dropna()\n",
    "\n",
    "# remove inputs that have constant value\n",
    "inputs = [c for c in inputs if df[c].nunique() > 1]\n",
    "\n",
    "# append system metrics if appropriate\n",
    "if 'hardware_type' in inputs:\n",
    "    inputs.remove('hardware_type')\n",
    "    inputs += [\n",
    "        'cpu_flops',\n",
    "        'cpu_mem_bw',\n",
    "        'disk_read',\n",
    "        'disk_write',\n",
    "        'gpu_flops',\n",
    "        'gpu_mem_bw',\n",
    "    ]\n",
    "\n",
    "# perform parameter sweep\n",
    "df_scores = []\n",
    "y_preds = {}\n",
    "\n",
    "hw_a = 'p100'\n",
    "hw_b = 'v100'\n",
    "n_rows_max = df['n_rows'].max()\n",
    "n_rows_values = sorted(df['n_rows'].unique())\n",
    "\n",
    "ab_values = [(a, b) for a in n_rows_values for b in n_rows_values[:-1]]\n",
    "names = ['%0.2f, %0.2f' % (a / n_rows_max, b / n_rows_max) for a, b in ab_values]\n",
    "\n",
    "for name, (n_rows_a, n_rows_b) in zip(names, ab_values):\n",
    "    # extract datasets for system A, system B\n",
    "    df_a = df[((df['hardware_type'] == hw_a) & (df['n_rows'] <= n_rows_a)) | ((df['hardware_type'] == hw_b) & (df['n_rows'] <= n_rows_b))]\n",
    "    df_b = df[(df['hardware_type'] == hw_b) & (df['n_rows'] > n_rows_b)]\n",
    "\n",
    "    # create train/test sets for df_a, df_b\n",
    "    X_train, y_train, columns, _ = create_dataset(df_a, inputs, target)\n",
    "    X_test, y_test, _, _ = create_dataset(df_b, inputs, target)\n",
    "\n",
    "    # define model\n",
    "    model = create_pipeline(create_mlp(X_train.shape[1], intervals=True))\n",
    "\n",
    "    # train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate model\n",
    "    y_bar, y_std = check_std(model.predict(X_test))\n",
    "    y_lower, y_upper = predict_intervals(y_bar, y_std)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_bar)\n",
    "    mpe = mean_absolute_percentage_error(y_test, y_bar)\n",
    "    cov = coverage_error(y_test, y_lower, y_upper)\n",
    "\n",
    "    # save metrics for plots\n",
    "    df_scores.append({\n",
    "        'name': name,\n",
    "        'mae': mae,\n",
    "        'mpe': mpe,\n",
    "        'cov': cov\n",
    "    })\n",
    "\n",
    "    # save predictions for plots\n",
    "    y_preds[name] = y_bar, y_std\n",
    "\n",
    "# save results\n",
    "df_scores = pd.DataFrame(df_scores)\n",
    "\n",
    "# plot evaluation scores for each model\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='name', y='mpe', data=df_scores, ci=68, color='tab:blue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('MAPE (%)')\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(plt.xlim(), [20, 20], 'r--')\n",
    "plt.xlim(xmin, xmax)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x='name', y='cov', data=df_scores, ci=68, color='tab:blue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Coverage Error (%)')\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(plt.xlim(), [5, 5], 'r--')\n",
    "plt.xlim(xmin, xmax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03-kinc-palmetto-palmetto-summary.pdf')\n",
    "plt.savefig('03-kinc-palmetto-palmetto-summary.png')\n",
    "plt.show()\n",
    "\n",
    "# plot expected vs predicted target values for each model\n",
    "fig, axes = plt.subplots(4, 3, figsize=(4 * 3, 4 * 4))\n",
    "\n",
    "for name, (n_rows_a, n_rows_b), ax in zip(names, ab_values, axes.flatten()):\n",
    "    # extract datasets for system A, system B\n",
    "    df_a = df[((df['hardware_type'] == hw_a) & (df['n_rows'] <= n_rows_a)) | ((df['hardware_type'] == hw_b) & (df['n_rows'] <= n_rows_b))]\n",
    "    df_b = df[(df['hardware_type'] == hw_b) & (df['n_rows'] > n_rows_b)]\n",
    "\n",
    "    # create train/test sets for df_a, df_b\n",
    "    X_train, y_train, columns, _ = create_dataset(df_a, inputs, target)\n",
    "    X_test, y_test, _, _ = create_dataset(df_b, inputs, target)\n",
    "\n",
    "    # get model predictions\n",
    "    y_bar, y_std = y_preds[name]\n",
    "    y_lower, y_upper = predict_intervals(y_bar, y_std)\n",
    "\n",
    "    # save model predictions\n",
    "    target_pred = '%s | %s' % (target, name)\n",
    "    df_b[target_pred] = y_bar\n",
    "\n",
    "    # save anomaly mask\n",
    "    anomaly_pred = 'anomaly | %s' % (name)\n",
    "    y_anomaly = anomaly_score(y_test, y_bar, y_std)\n",
    "    df_b[anomaly_pred] = (np.abs(y_anomaly) > 0.997)\n",
    "\n",
    "    # compute error bars\n",
    "    yerr = np.stack([\n",
    "        y_bar - y_lower,\n",
    "        y_upper - y_bar\n",
    "    ])\n",
    "\n",
    "    # create scatterplot\n",
    "    mask = ~df_b[anomaly_pred]\n",
    "    ax.errorbar(\n",
    "        x=target,\n",
    "        y=target_pred,\n",
    "        yerr=yerr[:, mask],\n",
    "        data=df_b[mask],\n",
    "        ecolor='tab:blue', c='tab:blue', ls='', marker='o', mec='w')\n",
    "\n",
    "    mask = df_b[anomaly_pred]\n",
    "    ax.errorbar(\n",
    "        x=target,\n",
    "        y=target_pred,\n",
    "        yerr=yerr[:, mask],\n",
    "        data=df_b[mask],\n",
    "        ecolor='tab:red', c='tab:red', ls='', marker='o', mec='w')\n",
    "\n",
    "    vmax = max(df_b[target].max(), df_b[target_pred].max())\n",
    "    ax.plot([0, vmax], [0, vmax], 'k--', zorder=0)\n",
    "    ax.set_xlabel(target)\n",
    "    ax.set_ylabel(target_pred)\n",
    "\n",
    "    cov = df_scores.loc[df_scores['name'] == name, 'cov']\n",
    "    ax.set_title('Coverage = %0.0f %%' % (100 - cov))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03-kinc-palmetto-palmetto-scatter.pdf')\n",
    "plt.savefig('03-kinc-palmetto-palmetto-scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palmetto/Nautilus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_name, target = 'similarity_chunk', 'runtime_hr'\n",
    "\n",
    "# get performance data for pipeline / process\n",
    "df_palmetto = data_map['breast-palmetto'][process_name]\n",
    "df_palmetto = df_palmetto[df_palmetto['hardware_type'] != 'cpu']\n",
    "df_nautilus_a = data_map['breast-nautilus'][process_name]\n",
    "df_nautilus_b = data_map['unified-nautilus'][process_name]\n",
    "\n",
    "df = pd.concat([\n",
    "    df_palmetto,\n",
    "    df_nautilus_a,\n",
    "    df_nautilus_b\n",
    "])\n",
    "inputs = config['train_inputs'][process_name]\n",
    "\n",
    "# remove samples with missing data\n",
    "df = df.dropna()\n",
    "\n",
    "# remove inputs that have constant value\n",
    "inputs = [c for c in inputs if df[c].nunique() > 1]\n",
    "\n",
    "# append system metrics if appropriate\n",
    "if 'hardware_type' in inputs:\n",
    "    inputs.remove('hardware_type')\n",
    "    inputs += [\n",
    "        'cpu_flops',\n",
    "        'cpu_mem_bw',\n",
    "        'disk_read',\n",
    "        'disk_write',\n",
    "        'gpu_flops',\n",
    "        'gpu_mem_bw',\n",
    "    ]\n",
    "\n",
    "# perform parameter sweep\n",
    "df_scores = []\n",
    "y_preds = {}\n",
    "\n",
    "n_rows_max = df_palmetto['n_rows'].max()\n",
    "n_rows_values = sorted(df_palmetto['n_rows'].unique())\n",
    "names = ['%0.2f' % (n_rows / n_rows_max) for n_rows in n_rows_values]\n",
    "\n",
    "for name, n_rows in zip(names, n_rows_values):\n",
    "    # extract datasets for system A, system B\n",
    "    df_a = df[((df['platform'] == 'palmetto') & (df['n_rows'] <= n_rows)) | ((df['platform'] == 'nautilus') & df['dataset'].str.startswith('breast.001'))]\n",
    "    df_b = df[(df['platform'] == 'nautilus') & (~df['dataset'].str.startswith('breast.001'))]\n",
    "\n",
    "    # create train/test sets for df_a, df_b\n",
    "    X_train, y_train, columns, _ = create_dataset(df_a, inputs, target)\n",
    "    X_test, y_test, _, _ = create_dataset(df_b, inputs, target)\n",
    "\n",
    "    # define model\n",
    "    model = create_pipeline(create_mlp(X_train.shape[1], intervals=True))\n",
    "\n",
    "    # train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate model\n",
    "    y_bar, y_std = check_std(model.predict(X_test))\n",
    "    y_lower, y_upper = predict_intervals(y_bar, y_std)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_bar)\n",
    "    mpe = mean_absolute_percentage_error(y_test, y_bar)\n",
    "    cov = coverage_error(y_test, y_lower, y_upper)\n",
    "\n",
    "    # save metrics for plots\n",
    "    df_scores.append({\n",
    "        'name': name,\n",
    "        'mae': mae,\n",
    "        'mpe': mpe,\n",
    "        'cov': cov\n",
    "    })\n",
    "\n",
    "    # save predictions for plots\n",
    "    y_preds[name] = y_bar, y_std\n",
    "\n",
    "# save results\n",
    "df_scores = pd.DataFrame(df_scores)\n",
    "\n",
    "# plot evaluation scores for each model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x='name', y='mae', data=df_scores, ci=68, color='tab:blue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('MAE (%s)' % (UNITS[target]))\n",
    "plt.plot(plt.xlim(), [df_b[target].median(), df_b[target].median()], 'r--')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x='name', y='mpe', data=df_scores, ci=68, color='tab:blue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.plot(plt.xlim(), [20, 20], 'r--')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x='name', y='cov', data=df_scores, ci=68, color='tab:blue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Coverage Error (%)')\n",
    "plt.plot(plt.xlim(), [5, 5], 'r--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot coverage profile for each model\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "for name, n_rows in zip(names, n_rows_values):\n",
    "    # extract datasets for system A, system B\n",
    "    df_a = df[((df['platform'] == 'palmetto') & (df['n_rows'] <= n_rows)) | ((df['platform'] == 'nautilus') & df['dataset'].str.startswith('breast.001'))]\n",
    "    df_b = df[(df['platform'] == 'nautilus') & (~df['dataset'].str.startswith('breast.001'))]\n",
    "\n",
    "    # create train/test sets for df_a, df_b\n",
    "    X_train, y_train, columns, _ = create_dataset(df_a, inputs, target)\n",
    "    X_test, y_test, _, _ = create_dataset(df_b, inputs, target)\n",
    "\n",
    "    # get model predictions\n",
    "    y_bar, y_std = y_preds[name]\n",
    "\n",
    "    # compute coverage profile\n",
    "    ci_values = np.arange(0.00, 1.00, 0.01)\n",
    "    cov_values = np.zeros_like(ci_values)\n",
    "\n",
    "    for i, ci in enumerate(ci_values):\n",
    "        y_lower, y_upper = predict_intervals(y_bar, y_std, ci=ci)\n",
    "        cov_values[i] = prediction_interval_coverage(y_test, y_lower, y_upper)\n",
    "\n",
    "    # plot coverage profile\n",
    "    plt.plot(100 * ci_values, 100 * cov_values, label=name)\n",
    "\n",
    "plt.plot([0, 100], [0, 100], 'k--', zorder=0)\n",
    "plt.legend(title='model')\n",
    "plt.xlabel('Confidence Interval (%)')\n",
    "plt.ylabel('Coverage (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot expected vs predicted target values for each model\n",
    "fig, axes = plt.subplots(1, len(names), figsize=(4 * len(names), 4), squeeze=False)\n",
    "\n",
    "for name, n_rows, ax in zip(names, n_rows_values, axes.flatten()):\n",
    "    # extract datasets for system A, system B\n",
    "    df_a = df[((df['platform'] == 'palmetto') & (df['n_rows'] <= n_rows)) | ((df['platform'] == 'nautilus') & df['dataset'].str.startswith('breast.001'))]\n",
    "    df_b = df[(df['platform'] == 'nautilus') & (~df['dataset'].str.startswith('breast.001'))]\n",
    "\n",
    "    # create train/test sets for df_a, df_b\n",
    "    X_train, y_train, columns, _ = create_dataset(df_a, inputs, target)\n",
    "    X_test, y_test, _, _ = create_dataset(df_b, inputs, target)\n",
    "\n",
    "    # get model predictions\n",
    "    y_bar, y_std = y_preds[name]\n",
    "    y_lower, y_upper = predict_intervals(y_bar, y_std)\n",
    "\n",
    "    # save model predictions\n",
    "    target_pred = '%s | %s' % (target, name)\n",
    "    df_b[target_pred] = y_bar\n",
    "\n",
    "    # save anomaly mask\n",
    "    anomaly_pred = 'anomaly | %s' % (name)\n",
    "    y_anomaly = anomaly_score(y_test, y_bar, y_std)\n",
    "    df_b[anomaly_pred] = (np.abs(y_anomaly) > 0.997)\n",
    "\n",
    "    # compute error bars\n",
    "    yerr = np.stack([\n",
    "        y_bar - y_lower,\n",
    "        y_upper - y_bar\n",
    "    ])\n",
    "\n",
    "    # create scatterplot\n",
    "    mask = ~df_b[anomaly_pred]\n",
    "    ax.errorbar(\n",
    "        x=target,\n",
    "        y=target_pred,\n",
    "        yerr=yerr[:, mask],\n",
    "        data=df_b[mask],\n",
    "        ecolor='tab:blue', c='tab:blue', ls='', marker='o', mec='w')\n",
    "\n",
    "    mask = df_b[anomaly_pred]\n",
    "    ax.errorbar(\n",
    "        x=target,\n",
    "        y=target_pred,\n",
    "        yerr=yerr[:, mask],\n",
    "        data=df_b[mask],\n",
    "        ecolor='tab:red', c='tab:red', ls='', marker='o', mec='w')\n",
    "\n",
    "    vmax = max(df_b[target].max(), df_b[target_pred].max())\n",
    "    ax.plot([0, vmax], [0, vmax], 'k--', zorder=0)\n",
    "    ax.set_xlabel(target)\n",
    "    ax.set_ylabel(target_pred)\n",
    "\n",
    "    cov = df_scores.loc[df_scores['name'] == name, 'cov']\n",
    "    ax.set_title('Coverage = %0.0f %%' % (100 - cov))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03-kinc-palmetto-nautilus-scatter.pdf')\n",
    "plt.savefig('03-kinc-palmetto-nautilus-scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tesseract)",
   "language": "python",
   "name": "tesseract"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
