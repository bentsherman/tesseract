{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train a neural network and random forest to predict queue time of jobs based on requested resources. Jobs are queried via `qstat` and then cleaned. The prediction models are trained on a sliding window of jobs, so that predictions are localized in time. Jobs with low queue time are removed to mitigate data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../bin')\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow import keras\n",
    "\n",
    "from train import \\\n",
    "    is_categorical, \\\n",
    "    create_dataset, \\\n",
    "    evaluate_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat -xf > qstat.table.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load qstat data\n",
    "qstat_file = open('qstat.table.txt')\n",
    "\n",
    "# parse qstat data into records\n",
    "rows = []\n",
    "job = {}\n",
    "key = None\n",
    "\n",
    "for line in qstat_file:\n",
    "    # remove trailing newline\n",
    "    line = line.rstrip('\\n')\n",
    "\n",
    "    # parse new job on job header\n",
    "    if line.startswith('Job Id'):\n",
    "        job['Job_Id'] = line.split(': ')[1]\n",
    "\n",
    "    # parse new job attribute on space indent\n",
    "    elif line.startswith('    '):\n",
    "        key, value = line.strip().split(' = ')\n",
    "        job[key] = value\n",
    "\n",
    "    # parse continued job attribute on tab indent\n",
    "    elif line.startswith('\\t'):\n",
    "        job[key] += line.strip()\n",
    "\n",
    "    # append complete job on blank line\n",
    "    elif line == '':\n",
    "        rows.append(job)\n",
    "        job = {}\n",
    "        key = None\n",
    "\n",
    "    # append extra text to job attribute if possible\n",
    "    elif key != None:\n",
    "        job[key] += line.strip()\n",
    "\n",
    "    # otherwise print warning\n",
    "    else:\n",
    "        print('warning: unable to parse line \\'%s\\'' % (line))\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(rows)\n",
    "df.set_index('Job_Id', inplace=True)\n",
    "\n",
    "# remove unused columns\n",
    "columns = [\n",
    "    'Job_Owner',\n",
    "    'session_id',\n",
    "    'queue',\n",
    "    'Resource_List.nodect',\n",
    "    'Resource_List.ncpus',\n",
    "    'Resource_List.mpiprocs',\n",
    "    'Resource_List.mem',\n",
    "    'Resource_List.ngpus',\n",
    "    'Resource_List.nphis',\n",
    "    'Resource_List.place',\n",
    "    'Resource_List.qcat',\n",
    "    'Resource_List.walltime',\n",
    "    'Resource_List.select',\n",
    "    'exec_vnode',\n",
    "    'job_state',\n",
    "    'ctime',\n",
    "    'etime',\n",
    "    'qtime',\n",
    "    'stime',\n",
    "    'mtime',\n",
    "    'resources_used.ncpus',\n",
    "    'resources_used.cput',\n",
    "    'resources_used.cpupercent',\n",
    "    'resources_used.mem',\n",
    "    'resources_used.vmem',\n",
    "    'resources_used.walltime'\n",
    "]\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "# select only running or finished jobs\n",
    "df = df[(df['job_state'] == 'R') | (df['job_state'] == 'F')]\n",
    "\n",
    "# convert temporal columns to datetime\n",
    "for column in ['ctime', 'etime', 'qtime', 'stime', 'mtime']:\n",
    "    df[column] = pd.to_datetime(df[column], infer_datetime_format=True)  \n",
    "\n",
    "# convert resource list, resources used columns to numerical\n",
    "df['Resource_List.mem'] = df['Resource_List.mem'].str.strip('gb').astype(int)\n",
    "df['Resource_List.walltime'] = df['Resource_List.walltime'].apply(lambda td: pd.Timedelta(td).total_seconds() / 3600)\n",
    "df['resources_used.walltime'] = df['resources_used.walltime'].apply(lambda td: pd.Timedelta(td).total_seconds())\n",
    "\n",
    "# compute queue time (minutes)\n",
    "df['queuetime'] = (df['stime'] - df['qtime']).apply(lambda td: td.total_seconds())\n",
    "df['queuetime'] /= 60\n",
    "\n",
    "# remove jobs with missing queue time\n",
    "df = df[~df['queuetime'].isna()]\n",
    "\n",
    "# compute walltime (sanity check)\n",
    "df['walltime']  = (df['mtime'] - df['stime']).apply(lambda td: td.total_seconds())\n",
    "\n",
    "# sort rows by qtime\n",
    "df = df.sort_values('qtime')\n",
    "\n",
    "# save dataframe to file\n",
    "df.to_csv('qstat.table.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('qstat.table.txt', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Queue Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ctime`: The time that the job was created.\n",
    "- `qtime`: The time that the job entered the current queue.\n",
    "- `etime`: The time that the job became eligible to run, i.e. in a queued state while residing in an execution queue.\n",
    "- `stime`: Timestamp; time when the job started execution.  Changes when job is restarted.\n",
    "- `mtime`: The time that the job was last modified, changed state, or changed locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the time span of the dataset\n",
    "df['qtime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute summary statistics of queue time\n",
    "df['queuetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of queue time\n",
    "data = df['queuetime']\n",
    "data = data[(10 < data) & (data < 10000)]\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "sns.histplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Queue Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove jobs with negligible queue time (reduce data imbalance)\n",
    "df = df[(10 < df['queuetime']) & (df['queuetime'] < 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sliding window\n",
    "n = 200\n",
    "stride = n // 2\n",
    "\n",
    "# define inputs, target\n",
    "inputs = [\n",
    "    'Resource_List.nodect',\n",
    "    'Resource_List.ncpus',\n",
    "    'Resource_List.mem',\n",
    "    'Resource_List.ngpus',\n",
    "    'Resource_List.walltime'\n",
    "]\n",
    "target = 'queuetime'\n",
    "\n",
    "# define models\n",
    "models = [\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(128, 128, 128))),\n",
    "    ('rf', create_rf(criterion='mse')),\n",
    "]\n",
    "\n",
    "# train models at each time window\n",
    "df_scores = []\n",
    "\n",
    "for i in range(0, len(df.index) - n, stride):\n",
    "    print(i)\n",
    "\n",
    "    # extract window of jobs\n",
    "    df_window = df.iloc[i:(i + n)]\n",
    "\n",
    "    # extract performance dataset\n",
    "    X, y, columns, _ = create_dataset(df_window, inputs, target)\n",
    "\n",
    "    # visualize queue time vs each input (with feature importances)\n",
    "    if True:\n",
    "        rf = create_rf(criterion='mse')\n",
    "        rf.fit(X, y)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(inputs), figsize=(4 * len(inputs), 4))\n",
    "\n",
    "        for ax, xaxis, importance in zip(axes, inputs, rf.feature_importances_):\n",
    "            ax.scatter(xaxis, target, data=df_window)\n",
    "            ax.set_xlabel(xaxis)\n",
    "            ax.set_ylabel(target)\n",
    "            ax.set_title('importance = %0.3f' % (importance))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # evaluate each model on dataset\n",
    "    y_preds = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        scores, y_bar, y_std = evaluate_cv(model, X, y)\n",
    "\n",
    "        df_scores.append({\n",
    "            'window_start': i,\n",
    "            'median': df_window[target].median(),\n",
    "            'name': name,\n",
    "            'mae': np.mean(scores['mae']),\n",
    "            'mpe': np.mean(scores['mpe'])\n",
    "        })\n",
    "        y_preds[name] = y_bar\n",
    "\n",
    "    # plot expected vs predicted target values\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(4 * len(models), 4), squeeze=False)\n",
    "\n",
    "    for (name, model), ax in zip(models, axes.flatten()):\n",
    "        y_pred = y_preds[name]\n",
    "\n",
    "        sns.scatterplot(x=y, y=y_pred, ax=ax)\n",
    "        vmax = max(y.max(), y_pred.max())\n",
    "        ax.plot([0, vmax], [0, vmax], 'k--')\n",
    "        ax.set_xlabel(target)\n",
    "        ax.set_ylabel('%s | %s' % (target, name))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# visualize prediction error over time\n",
    "df_scores = pd.DataFrame(df_scores)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.lineplot(x='window_start', y='mae', hue='name', data=df_scores)\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(df_scores['window_start'], df_scores['median'], 'r--', label='median')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.lineplot(x='window_start', y='mpe', hue='name', data=df_scores)\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot([xmin, xmax], [100, 100], 'r--', label='100 %')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tesseract)",
   "language": "python",
   "name": "tesseract"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
