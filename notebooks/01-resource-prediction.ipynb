{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../bin')\n",
    "\n",
    "from IPython.display import display\n",
    "import json\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "from tensorflow import keras\n",
    "\n",
    "from utils import \\\n",
    "    UNITS, \\\n",
    "    anomaly_score, \\\n",
    "    check_std, \\\n",
    "    predict_intervals\n",
    "\n",
    "from train import \\\n",
    "    load_datasets, \\\n",
    "    is_categorical, \\\n",
    "    create_dataset, \\\n",
    "    create_gb, \\\n",
    "    create_lr, \\\n",
    "    asym_loss, \\\n",
    "    create_mlp, \\\n",
    "    create_rf, \\\n",
    "    create_pipeline, \\\n",
    "    mean_absolute_percentage_error, \\\n",
    "    prediction_interval_coverage, \\\n",
    "    coverage_error, \\\n",
    "    evaluate_trials, \\\n",
    "    evaluate_cv\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "    'gemmaker',\n",
    "    'gene-oracle',\n",
    "    'hemelb',\n",
    "    'kinc',\n",
    "    'tspg'\n",
    "]\n",
    "\n",
    "config_map = {p: json.load(open('../workflows/%s/params.json' % (p))) for p in pipelines}\n",
    "data_map = {}\n",
    "\n",
    "for p, c in config_map.items():\n",
    "    process_names = c['train_inputs'].keys()\n",
    "    merge_files = [arg.split(' ') for arg in c['train_merge_args']]\n",
    "\n",
    "    data_map[p] = load_datasets(p, process_names, base_dir='../_datasets', merge_files=merge_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline, dfs in data_map.items():\n",
    "    for process_name, df in dfs.items():\n",
    "        print('%16s %24s %8d' % (pipeline, process_name, len(df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline, dfs in data_map.items():\n",
    "    for process_name, df in dfs.items():\n",
    "        print(pipeline, process_name)\n",
    "        print()\n",
    "        display(df)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Prediction Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = []\n",
    "\n",
    "for pipeline, dfs in data_map.items():\n",
    "    config = config_map[pipeline]\n",
    "\n",
    "    for process_name, df in dfs.items():\n",
    "        inputs = config['train_inputs'][process_name]\n",
    "\n",
    "        for target in ['runtime_hr', 'memory_GB', 'disk_GB']:\n",
    "            df_stats.append({\n",
    "                'pipeline': pipeline,\n",
    "                'process_name': process_name,\n",
    "                'target': target,\n",
    "                'min': df[target].min(),\n",
    "                'median': df[target].median(),\n",
    "                'max': df[target].max(),\n",
    "                'mean': df[target].mean(),\n",
    "                'std': df[target].std()\n",
    "            })\n",
    "\n",
    "df_stats = pd.DataFrame(df_stats)\n",
    "\n",
    "display(df_stats)\n",
    "\n",
    "targets_incl = [(r.pipeline, r.process_name, r.target) for i, r in df_stats[df_stats['std'] > 0.1].iterrows()]\n",
    "targets_excl = [(r.pipeline, r.process_name, r.target) for i, r in df_stats[df_stats['std'] < 0.1].iterrows()]\n",
    "\n",
    "print('Selected %d prediction targets' % (len(targets_incl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(x_axes, y_axes, data, titles=None, outfile=None, **kwargs):\n",
    "    fig, axes = plt.subplots(\n",
    "        len(x_axes), len(y_axes),\n",
    "        figsize=(4 * len(y_axes), 4 * len(x_axes)),\n",
    "        squeeze=False,\n",
    "        **kwargs)\n",
    "\n",
    "    for i, x in enumerate(x_axes):\n",
    "        for j, y in enumerate(y_axes):\n",
    "            ax = axes[i][j]\n",
    "\n",
    "            if x == None:\n",
    "                sns.histplot(data[y], ax=ax)\n",
    "            elif is_categorical(data, x):\n",
    "                sns.stripplot(x=x, y=y, data=data, ax=ax)\n",
    "            else:\n",
    "                sns.scatterplot(x=x, y=y, data=data, ax=ax)\n",
    "\n",
    "            if i == 0 and titles != None:\n",
    "                ax.set_title(titles[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if outfile != None:\n",
    "        plt.savefig(outfile)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Excluded Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize subplots\n",
    "n_cols = 3\n",
    "n_rows = 4\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "# flatten list of axes\n",
    "axes = [ax for i in range(len(axes)) for ax in axes[i]]\n",
    "\n",
    "# sample from excluded prediction targets\n",
    "targets_sampled = random.sample(targets_excl, n_rows * n_cols)\n",
    "\n",
    "for (pipeline, process_name, target), ax in zip(targets_sampled, axes):\n",
    "    # get target data\n",
    "    df = data_map[pipeline][process_name]\n",
    "    y_true = df[target]\n",
    "\n",
    "    # compute mean and std\n",
    "    y_bar = np.mean(y_true)\n",
    "    y_std = np.std(y_true)\n",
    "    y_err = 2.0 * y_std\n",
    "\n",
    "    # get reccomended resource request\n",
    "    y_recc = np.ceil(np.max(y_true))\n",
    "\n",
    "    # plot target data with mean and 95% CI\n",
    "    ax.plot(np.arange(len(y_true)), y_true, ls='', marker='.')\n",
    "    ax.plot([0, len(y_true) - 1], [y_bar, y_bar], 'r--')\n",
    "    ax.plot([0, len(y_true) - 1], [y_bar + y_err, y_bar + y_err], 'r:')\n",
    "    ax.plot([0, len(y_true) - 1], [y_bar - y_err, y_bar - y_err], 'r:')\n",
    "    ax.set_title('ceil(max) = %0.0f %s' % (y_recc, UNITS[target]))\n",
    "    ax.set_ylabel('%s / %s / %s' % (pipeline, process_name, target))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01-excluded-targets.pdf')\n",
    "plt.savefig('01-excluded-targets.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Selected Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline, process_name, target in targets_incl:\n",
    "    print()\n",
    "    print(pipeline, process_name, target)\n",
    "\n",
    "    # get performance data for pipeline / process\n",
    "    df = data_map[pipeline][process_name]\n",
    "    inputs = config_map[pipeline]['train_inputs'][process_name]\n",
    "\n",
    "    # remove inputs that have constant value\n",
    "    inputs = [c for c in inputs if df[c].nunique() > 1]\n",
    "\n",
    "    # skip if there are no input features\n",
    "    if len(inputs) == 0:\n",
    "        print('no input features, skipping')\n",
    "        continue\n",
    "\n",
    "    # extract performance dataset\n",
    "    X, y, columns, _ = create_dataset(df, inputs, target)\n",
    "\n",
    "    # define models\n",
    "    models = [\n",
    "        ('mlp', create_mlp(X.shape[1])),\n",
    "        ('rf', create_rf()),\n",
    "    ]\n",
    "\n",
    "    # prepend scaler to each model\n",
    "    models = [(name, create_pipeline(model)) for name, model in models]\n",
    "\n",
    "    # evaluate each model on dataset\n",
    "    df_scores = []\n",
    "    y_preds = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        # evaluate model\n",
    "        scores, y_bar, y_std = evaluate_cv(model, X, y)\n",
    "\n",
    "        # save metrics for plots\n",
    "        df_scores.append({\n",
    "            'name': name,\n",
    "            'mae': scores['mae'],\n",
    "            'mpe': scores['mpe'],\n",
    "            'cov': scores['cov']\n",
    "        })\n",
    "\n",
    "        # save predictions for plots\n",
    "        y_preds[name] = y_bar, y_std\n",
    "\n",
    "    # save results\n",
    "    df_scores = pd.DataFrame(df_scores)\n",
    "\n",
    "    # plot evaluation scores for each model\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.barplot(x='name', y='mae', data=df_scores, ci=68, color='tab:blue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('MAE (%s)' % (UNITS[target]))\n",
    "    plt.plot(plt.xlim(), [df.median(), df.median()], 'r--')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.barplot(x='name', y='mpe', data=df_scores, ci=68, color='tab:blue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('MAPE (%)')\n",
    "    plt.plot(plt.xlim(), [20, 20], 'r--')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.barplot(x='name', y='cov', data=df_scores, ci=68, color='tab:blue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Coverage Error (%)')\n",
    "    plt.plot(plt.xlim(), [5, 5], 'r--')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plot expected vs predicted target values for each model\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(4 * len(models), 4))\n",
    "\n",
    "    for (name, model), ax in zip(models, axes):\n",
    "        # get model predictions\n",
    "        y_bar, y_std = y_preds[name]\n",
    "\n",
    "        # save model predictions\n",
    "        target_pred = '%s | %s' % (target, name)\n",
    "        df[target_pred] = y_bar\n",
    "\n",
    "        # create scatterplot\n",
    "        ax.errorbar(\n",
    "            x=target,\n",
    "            y=target_pred,\n",
    "            data=df,\n",
    "            ecolor='tab:blue', c='tab:blue', ls='', marker='o', mec='w')\n",
    "\n",
    "        vmax = max(df[target].max(), df[target_pred].max())\n",
    "        ax.plot([0, vmax], [0, vmax], 'k--', zorder=0)\n",
    "        ax.plot([0, vmax / 1.2], [0, vmax], 'r--', zorder=0)\n",
    "        ax.plot([0, vmax], [0, vmax / 1.2], 'r--', zorder=0)\n",
    "        ax.set_xlabel(target)\n",
    "        ax.set_ylabel(target_pred)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plot side-by-side of each input feature\n",
    "    x_axes = inputs\n",
    "    y_axes = [target] + ['%s | %s' % (target, name) for name, model in models]\n",
    "    titles = ['Expected'] + ['Predicted (%s)' % (name) for name, model in models]\n",
    "    data = df\n",
    "\n",
    "    make_plots(\n",
    "        inputs,\n",
    "        y_axes,\n",
    "        data,\n",
    "        titles=titles,\n",
    "        sharey='row')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Selected Targets (with Intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = []\n",
    "\n",
    "for pipeline, process_name, target in targets_incl:\n",
    "    print()\n",
    "    print(pipeline, process_name, target)\n",
    "\n",
    "    # get performance data for pipeline / process\n",
    "    df = data_map[pipeline][process_name]\n",
    "    inputs = config_map[pipeline]['train_inputs'][process_name]\n",
    "\n",
    "    # remove inputs that have constant value\n",
    "    inputs = [c for c in inputs if df[c].nunique() > 1]\n",
    "\n",
    "    # skip if there are no input features\n",
    "    if len(inputs) == 0:\n",
    "        print('no input features, skipping')\n",
    "        continue\n",
    "\n",
    "    # compute summary statistics\n",
    "    row = {\n",
    "        'pipeline': pipeline,\n",
    "        'process': process_name,\n",
    "        'target': target,\n",
    "        'median': df[target].median()\n",
    "    }\n",
    "\n",
    "    # extract performance dataset\n",
    "    X, y, columns, _ = create_dataset(df, inputs, target)\n",
    "\n",
    "    # define models\n",
    "    models = [\n",
    "        ('mlp', create_mlp(X.shape[1], intervals=True)),\n",
    "        ('rf', create_rf(intervals=True)),\n",
    "    ]\n",
    "\n",
    "    # prepend scaler to each model\n",
    "    models = [(name, create_pipeline(model)) for name, model in models]\n",
    "\n",
    "    # evaluate each model on dataset\n",
    "    df_scores = []\n",
    "    y_preds = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        # evaluate model\n",
    "        scores, y_bar, y_std = evaluate_cv(model, X, y)\n",
    "\n",
    "        # save metrics for results dataframe\n",
    "        row['%s | mae' % (name)] = scores['mae']\n",
    "        row['%s | mpe' % (name)] = scores['mpe']\n",
    "        row['%s | cov' % (name)] = scores['cov']\n",
    "\n",
    "        # save metrics for plots\n",
    "        df_scores.append({\n",
    "            'name': name,\n",
    "            'mae': scores['mae'],\n",
    "            'mpe': scores['mpe'],\n",
    "            'cov': scores['cov']\n",
    "        })\n",
    "\n",
    "        # save predictions for plots\n",
    "        y_preds[name] = y_bar, y_std\n",
    "\n",
    "    # save results\n",
    "    df_results.append(row)\n",
    "    df_scores = pd.DataFrame(df_scores)\n",
    "\n",
    "    # plot evaluation scores for each model\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.barplot(x='name', y='mae', data=df_scores, ci=68, color='tab:blue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('MAE (%s)' % (UNITS[target]))\n",
    "    plt.plot(plt.xlim(), [row['median'], row['median']], 'r--')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.barplot(x='name', y='mpe', data=df_scores, ci=68, color='tab:blue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('MAPE (%)')\n",
    "    plt.plot(plt.xlim(), [20, 20], 'r--')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.barplot(x='name', y='cov', data=df_scores, ci=68, color='tab:blue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Coverage Error (%)')\n",
    "    plt.plot(plt.xlim(), [5, 5], 'r--')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plot coverage profile for each model\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for name, model in models:\n",
    "        # get model predictions\n",
    "        y_bar, y_std = y_preds[name]\n",
    "\n",
    "        # compute coverage profile\n",
    "        ci_values = np.arange(0.00, 1.00, 0.01)\n",
    "        cov_values = np.zeros_like(ci_values)\n",
    "\n",
    "        for i, ci in enumerate(ci_values):\n",
    "            y_lower, y_upper = predict_intervals(y_bar, y_std, ci=ci)\n",
    "            cov_values[i] = prediction_interval_coverage(y, y_lower, y_upper)\n",
    "\n",
    "        # plot coverage profile\n",
    "        plt.plot(100 * ci_values, 100 * cov_values, label=name)\n",
    "\n",
    "    plt.plot([0, 100], [0, 100], 'k--', zorder=0)\n",
    "    plt.legend(title='model')\n",
    "    plt.xlabel('Confidence Interval (%)')\n",
    "    plt.ylabel('Coverage (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plot expected vs predicted target values for each model\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(4 * len(models), 4))\n",
    "\n",
    "    for (name, model), ax in zip(models, axes):\n",
    "        # get model predictions\n",
    "        y_bar, y_std = y_preds[name]\n",
    "        y_lower, y_upper = predict_intervals(y_bar, y_std)\n",
    "\n",
    "        # save model predictions\n",
    "        target_pred = '%s | %s' % (target, name)\n",
    "        df[target_pred] = y_bar\n",
    "\n",
    "        # save anomaly mask\n",
    "        anomaly_pred = 'anomaly | %s' % (name)\n",
    "        y_anomaly = anomaly_score(y, y_bar, y_std)\n",
    "        df[anomaly_pred] = (np.abs(y_anomaly) > 0.997)\n",
    "\n",
    "        # compute error bars\n",
    "        yerr = np.stack([\n",
    "            y_bar - y_lower,\n",
    "            y_upper - y_bar\n",
    "        ])\n",
    "\n",
    "        # create scatterplot\n",
    "        mask = ~df[anomaly_pred]\n",
    "        ax.errorbar(\n",
    "            x=target,\n",
    "            y=target_pred,\n",
    "            yerr=yerr[:, mask],\n",
    "            data=df[mask],\n",
    "            ecolor='tab:blue', c='tab:blue', ls='', marker='o', mec='w')\n",
    "\n",
    "        mask = df[anomaly_pred]\n",
    "        ax.errorbar(\n",
    "            x=target,\n",
    "            y=target_pred,\n",
    "            yerr=yerr[:, mask],\n",
    "            data=df[mask],\n",
    "            ecolor='tab:red', c='tab:red', ls='', marker='o', mec='w')\n",
    "\n",
    "        vmax = max(df[target].max(), df[target_pred].max())\n",
    "        ax.plot([0, vmax], [0, vmax], 'k--', zorder=0)\n",
    "        ax.set_xlabel(target)\n",
    "        ax.set_ylabel(target_pred)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s-%s-%s-scatter.pdf' % (pipeline, process_name, target))\n",
    "    plt.savefig('%s-%s-%s-scatter.png' % (pipeline, process_name, target))\n",
    "    plt.show()\n",
    "\n",
    "    # categorize anomalies\n",
    "    df['anomaly'] = df['anomaly | mlp'] | df['anomaly | rf']\n",
    "    df['anomaly_type'] = 'none'\n",
    "    df.loc[ df['anomaly | mlp'] & ~df['anomaly | rf'], 'anomaly_type'] = 'mlp'\n",
    "    df.loc[~df['anomaly | mlp'] &  df['anomaly | rf'], 'anomaly_type'] = 'rf'\n",
    "    df.loc[ df['anomaly | mlp'] &  df['anomaly | rf'], 'anomaly_type'] = 'mlp+rf'\n",
    "\n",
    "    # plot side-by-side of each input feature\n",
    "    x_axes = inputs\n",
    "    y_axes = [target] + ['%s | %s' % (target, name) for name, model in models]\n",
    "    titles = ['Expected'] + ['Predicted (%s)' % (name) for name, model in models]\n",
    "    data = df\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        len(x_axes), len(y_axes),\n",
    "        figsize=(4 * len(y_axes), 4 * len(x_axes)),\n",
    "        squeeze=False,\n",
    "        sharey='row')\n",
    "\n",
    "    for i, x in enumerate(x_axes):\n",
    "        for j, y in enumerate(y_axes):\n",
    "            ax = axes[i][j]\n",
    "\n",
    "            if j == 0:\n",
    "                hue = 'anomaly_type'\n",
    "                hue_order = ['none', 'mlp', 'rf', 'mlp+rf']\n",
    "                sizes = {'none': 30, 'mlp': 120, 'rf': 120, 'mlp+rf': 120}\n",
    "                markers = {'none': 'o', 'mlp': 'X', 'rf': 'X', 'mlp+rf': 'X'}\n",
    "                palette = {'none': 'tab:blue', 'mlp': 'tab:orange', 'rf': 'tab:red', 'mlp+rf': 'tab:pink'}\n",
    "            elif j == 1:\n",
    "                hue = 'anomaly | mlp'\n",
    "                hue_order = None\n",
    "                sizes = {False: 30, True: 120}\n",
    "                markers = {False: 'o', True: 'X'}\n",
    "                palette = {False: 'tab:blue', True: 'tab:orange'}\n",
    "            elif j == 2:\n",
    "                hue = 'anomaly | rf'\n",
    "                hue_order = None\n",
    "                sizes = {False: 30, True: 120}\n",
    "                markers = {False: 'o', True: 'X'}\n",
    "                palette = {False: 'tab:blue', True: 'tab:red'}\n",
    "\n",
    "            if is_categorical(data, x):\n",
    "                sns.stripplot(x=x, y=y, hue=hue, hue_order=hue_order, data=data, dodge=True, palette=palette, ax=ax)\n",
    "            else:\n",
    "                sns.scatterplot(x=x, y=y, hue=hue, hue_order=hue_order, size=hue, sizes=sizes, style=hue, markers=markers, data=data, palette=palette, ax=ax)\n",
    "\n",
    "            if i == 0 and titles != None:\n",
    "                ax.set_title(titles[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s-%s-%s-marginals.pdf' % (pipeline, process_name, target))\n",
    "    plt.savefig('%s-%s-%s-marginals.png' % (pipeline, process_name, target))\n",
    "    plt.show()\n",
    "\n",
    "# save results to dataframe\n",
    "df_results = pd.DataFrame(df_results)\n",
    "df_results.set_index(['pipeline', 'process', 'target'], inplace=True)\n",
    "df_results.to_csv('01-resource-prediction.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = []\n",
    "\n",
    "for pipeline, process_name, target in targets_incl:\n",
    "    print()\n",
    "    print(pipeline, process_name, target)\n",
    "\n",
    "    # get performance data for pipeline / process\n",
    "    df = data_map[pipeline][process_name]\n",
    "    inputs = config_map[pipeline]['train_inputs'][process_name]\n",
    "\n",
    "    # remove inputs that have constant value\n",
    "    inputs = [c for c in inputs if df[c].nunique() > 1]\n",
    "\n",
    "    # skip if there are no input features\n",
    "    if len(inputs) == 0:\n",
    "        print('no input features, skipping')\n",
    "        continue\n",
    "\n",
    "    # extract performance dataset\n",
    "    X, y, columns, _ = create_dataset(df, inputs, target)\n",
    "\n",
    "    # create model\n",
    "    model = create_pipeline(create_mlp(X.shape[1], intervals=True))\n",
    "\n",
    "    # evaluate each train/test split\n",
    "    train_sizes = np.arange(0.1, 1.0, 0.1)\n",
    "    scores_map = evaluate_trials(model, X, y, train_sizes=train_sizes, n_trials=3)\n",
    "\n",
    "    # collect scores\n",
    "    df_scores = []\n",
    "    min_train_size_mpe = 1.0\n",
    "    min_train_size_cov = 1.0\n",
    "\n",
    "    for train_size in train_sizes:\n",
    "        # get scores for this split\n",
    "        scores = scores_map[train_size]\n",
    "\n",
    "        # update minimum samples\n",
    "        if np.mean(scores['mpe']) <= 20:\n",
    "            min_train_size_mpe = min(train_size, min_train_size_mpe)\n",
    "\n",
    "        if np.mean(scores['cov']) <= 5:\n",
    "            min_train_size_cov = min(train_size, min_train_size_cov)\n",
    "\n",
    "        # save metrics\n",
    "        scores = pd.DataFrame({\n",
    "            'name': '%d / %d' % (X.shape[0] * train_size, X.shape[0]),\n",
    "            'mpe': scores['mpe'],\n",
    "            'cov': scores['cov']\n",
    "        })\n",
    "        df_scores.append(scores)\n",
    "\n",
    "    df_scores = pd.concat(df_scores)\n",
    "\n",
    "    # save minimum samples\n",
    "    df_results.append({\n",
    "        'name': '%s / %s / %s' % (pipeline, process_name, target),\n",
    "        'model': 'mlp',\n",
    "        'min_samples_mpe': min_train_size_mpe * X.shape[0],\n",
    "        'min_samples_cov': min_train_size_cov * X.shape[0],\n",
    "        'n_samples': X.shape[0],\n",
    "    })\n",
    "\n",
    "    # plot results\n",
    "    plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(x='name', y='mpe', data=df_scores, color='tab:blue')\n",
    "    xmin, xmax = plt.xlim()\n",
    "    plt.plot([xmin, xmax], [20, 20], 'r--')\n",
    "    plt.xlabel('Training Samples / Total Samples')\n",
    "    plt.ylabel('MAPE (%)')\n",
    "    plt.xlim(xmin, xmax)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.barplot(x='name', y='cov', data=df_scores, color='tab:blue')\n",
    "    xmin, xmax = plt.xlim()\n",
    "    plt.plot([xmin, xmax], [5, 5], 'r--')\n",
    "    plt.xlabel('Training Samples / Total Samples')\n",
    "    plt.ylabel('Coverage Error (%)')\n",
    "    plt.xlim(xmin, xmax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s-%s-%s-trainsize.pdf' % (pipeline, process_name, target))\n",
    "    plt.savefig('%s-%s-%s-trainsize.png' % (pipeline, process_name, target))\n",
    "    plt.show()\n",
    "\n",
    "# save results to dataframe\n",
    "df_results = pd.DataFrame(df_results)\n",
    "df_results.set_index('name', inplace=True)\n",
    "df_results.to_csv('01-train-size.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load resource prediction results\n",
    "df = pd.read_csv('01-resource-prediction.csv', sep='\\t')\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    df.loc[idx, 'name'] = '%s / %s / %s' % (row.pipeline, row.process, row.target)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 12), sharey=True)\n",
    "\n",
    "# plot mean relative error for each prediction target\n",
    "ax = axes[0]\n",
    "data = df.copy()\n",
    "data['20'] = 20\n",
    "sns.pointplot(y='name', x='20', data=data, color='tab:red', markers='', linestyles='--', ax=ax)\n",
    "\n",
    "data['mlp'] = data['mlp | mpe']\n",
    "data['rf'] = data['rf | mpe']\n",
    "data = data.melt(id_vars=['name'], value_vars=['mlp', 'rf'], var_name='model', value_name='mpe')\n",
    "sns.barplot(y='name', x='mpe', hue='model', data=data, ax=ax)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_xlabel('MAPE (%)')\n",
    "ax.set_ylabel('Name')\n",
    "\n",
    "# plot coverage error for aech prediction target\n",
    "ax = axes[1]\n",
    "data = df.copy()\n",
    "data['5'] = 5\n",
    "sns.pointplot(y='name', x='5', data=data, color='tab:red', markers='', linestyles='--', ax=ax)\n",
    "\n",
    "data['mlp'] = data['mlp | cov']\n",
    "data['rf'] = data['rf | cov']\n",
    "data = data.melt(id_vars=['name'], value_vars=['mlp', 'rf'], var_name='model', value_name='cov')\n",
    "sns.barplot(y='name', x='cov', hue='model', data=data, ax=ax)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_xlabel('Coverage Error (%)')\n",
    "ax.set_ylabel('Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01-resource-prediction.pdf')\n",
    "plt.savefig('01-resource-prediction.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training size results\n",
    "df = pd.read_csv('01-train-size.csv', sep='\\t')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 12), sharey=True)\n",
    "\n",
    "# plot minimum samples required (mpe) for each target\n",
    "ax = axes[0]\n",
    "sns.pointplot(y='name', x='n_samples', data=df, color='tab:red', markers='x', linestyles='', ax=ax)\n",
    "sns.barplot(y='name', x='min_samples_mpe', hue='model', data=df, zorder=-1, ax=ax)\n",
    "ax.set_xlabel('Minimum Samples (MAPE)')\n",
    "ax.set_ylabel('Name')\n",
    "\n",
    "# plot minimum samples required (cov) for each target\n",
    "ax = axes[1]\n",
    "sns.pointplot(y='name', x='n_samples', data=df, color='tab:red', markers='x', linestyles='', ax=ax)\n",
    "sns.barplot(y='name', x='min_samples_cov', hue='model', data=df, zorder=-1, ax=ax)\n",
    "ax.set_xlabel('Minimum Samples (Coverage Error)')\n",
    "ax.set_ylabel('Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01-train-size.pdf')\n",
    "plt.savefig('01-train-size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate MLP (with Training History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline, process_name, target in targets_incl:\n",
    "    print()\n",
    "    print(pipeline, process_name, target)\n",
    "\n",
    "    # get performance data for pipeline / process\n",
    "    df = data_map[pipeline][process_name]\n",
    "    inputs = config_map[pipeline]['train_inputs'][process_name]\n",
    "\n",
    "    # remove inputs that have constant value\n",
    "    inputs = [c for c in inputs if df[c].nunique() > 1]\n",
    "\n",
    "    # skip if there are no input features\n",
    "    if len(inputs) == 0:\n",
    "        print('no input features, skipping')\n",
    "        continue\n",
    "\n",
    "    # extract performance dataset\n",
    "    X, y, columns, _ = create_dataset(df, inputs, target)\n",
    "\n",
    "    # create train/test split\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # normalize data\n",
    "    scaler = sklearn.preprocessing.MaxAbsScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # train model\n",
    "    model = create_mlp(X.shape[1], intervals=True)\n",
    "    history = model.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate model\n",
    "    y_bar, y_std = check_std(model.predict(X_test))\n",
    "    y_lower, y_upper = predict_intervals(y_bar, y_std)\n",
    "\n",
    "    mae = sklearn.metrics.mean_absolute_error(y_test, y_bar)\n",
    "    mpe = mean_absolute_percentage_error(y_test, y_bar)\n",
    "    cov = coverage_error(y_test, y_lower, y_upper)\n",
    "\n",
    "    print()\n",
    "    print('mae: %0.3f %s' % (mae, UNITS[target]))\n",
    "    print('mpe: %0.3f %%' % (mpe))\n",
    "    print('cov: %0.3f %%' % (cov))\n",
    "\n",
    "    # generate predictions for entire dataset\n",
    "    model = create_pipeline(create_mlp(X.shape[1], intervals=True))\n",
    "    _, y_bar, y_std = evaluate_cv(model, X, y)\n",
    "    y_lower, y_upper = predict_intervals(y_bar, y_std)\n",
    "\n",
    "    target_pred = '%s | mlp' % (target)\n",
    "    df[target_pred] = y_bar\n",
    "\n",
    "    plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # plot training history\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Training History')\n",
    "    plt.ylabel('MAE (%s)' % (UNITS[target]))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    # plot expected vs predicted target values\n",
    "    cov = coverage_error(y, y_lower, y_upper)\n",
    "    yerr = [\n",
    "        y_bar - y_lower,\n",
    "        y_upper - y_bar\n",
    "    ]\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.errorbar(\n",
    "        x=target,\n",
    "        y=target_pred,\n",
    "        yerr=yerr,\n",
    "        data=df,\n",
    "        ecolor='tab:blue', c='tab:blue', ls='', marker='o', mec='w')\n",
    "    vmax = max(df[target].max(), df[target_pred].max())\n",
    "    plt.plot([0, vmax], [0, vmax], 'k--', zorder=0)\n",
    "    plt.title('cov = %0.3f %%' % (cov))\n",
    "    plt.xlabel(target)\n",
    "    plt.ylabel(target_pred)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Specialized Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_configs = {\n",
    "    'download_runs': {\n",
    "        'x': 'n_spots',\n",
    "        'hue': 'n_remote_run_ids',\n",
    "        'row': None\n",
    "    },\n",
    "    'fastq_dump': {\n",
    "        'x': 'sra_bytes',\n",
    "        'hue': None,\n",
    "        'row': None\n",
    "    },\n",
    "    'hemelb': {\n",
    "        'x': 'n_sites',\n",
    "        'hue': 'np',\n",
    "        'row': 'hardware_type'\n",
    "    },\n",
    "    'similarity_chunk': {\n",
    "        'x': 'n_rows',\n",
    "        'hue': 'chunks',\n",
    "        'row': 'hardware_type'\n",
    "    },\n",
    "    'similarity_merge': {\n",
    "        'x': 'n_rows',\n",
    "        'hue': 'n_cols',\n",
    "        'row': None\n",
    "    },\n",
    "    'similarity_mpi': {\n",
    "        'x': 'n_rows',\n",
    "        'hue': 'np',\n",
    "        'row': 'hardware_type'\n",
    "    },\n",
    "    'corrpower': {\n",
    "        'x': 'n_rows',\n",
    "        'hue': 'n_cols',\n",
    "        'row': None\n",
    "    }\n",
    "}\n",
    "\n",
    "for pipeline, process_name, target in targets_incl:\n",
    "    print()\n",
    "    print(pipeline, process_name, target)\n",
    "\n",
    "    # get performance data for pipeline / process\n",
    "    df = data_map[pipeline][process_name]\n",
    "    inputs = config_map[pipeline]['train_inputs'][process_name]\n",
    "\n",
    "    # remove inputs that have constant value\n",
    "    inputs = [c for c in inputs if df[c].nunique() > 1]\n",
    "\n",
    "    # skip if there are no input features\n",
    "    if len(inputs) == 0:\n",
    "        print('no input features, skipping')\n",
    "        continue\n",
    "\n",
    "    # skip if there is no plots config\n",
    "    if process_name not in plot_configs:\n",
    "        print('no plots config, skipping')\n",
    "        continue\n",
    "\n",
    "    # plot target data by itself\n",
    "    config = plot_configs[process_name]\n",
    "    x   = config['x']\n",
    "    hue = config['hue']\n",
    "    row = config['row']\n",
    "\n",
    "    if row != None:\n",
    "        df.sort_values(by=row, inplace=True, kind='mergesort')\n",
    "\n",
    "    if df[x].dtype.kind in 'biuOSUV':\n",
    "        plot_func = sns.stripplot\n",
    "    else:\n",
    "        plot_func = sns.scatterplot\n",
    "\n",
    "    g = sns.FacetGrid(\n",
    "        df,\n",
    "        row=row,\n",
    "        sharex=True,\n",
    "        sharey='row',\n",
    "        height=3,\n",
    "        aspect=2,\n",
    "        margin_titles=True\n",
    "    )\n",
    "    g.map_dataframe(\n",
    "        plot_func,\n",
    "        x=x,\n",
    "        y=target,\n",
    "        hue=hue,\n",
    "        palette='viridis',\n",
    "        dodge=True\n",
    "    )\n",
    "    g.set_axis_labels(x, target)\n",
    "    g.add_legend(title=hue)\n",
    "    plt.savefig('%s-%s-%s.pdf' % (pipeline, process_name, target))\n",
    "    plt.savefig('%s-%s-%s.png' % (pipeline, process_name, target))\n",
    "    plt.show()\n",
    "\n",
    "    continue\n",
    "\n",
    "    # extract performance dataset\n",
    "    X, y, columns, _ = create_dataset(df, inputs, target)\n",
    "\n",
    "    # train model\n",
    "    model = create_pipeline(create_mlp(X.shape[1]))\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # create dataframe of model predictions\n",
    "    df_true = df\n",
    "    df_pred = df.copy()\n",
    "    df_pred[target] = model.predict(X)\n",
    "\n",
    "    # create merged dataframe\n",
    "    df_true['data_type'] = 'true'\n",
    "    df_pred['data_type'] = 'pred'\n",
    "    data = pd.concat([df_true, df_pred])\n",
    "\n",
    "    # create facet grid of data and model distributions\n",
    "    config = plot_configs[process_name]\n",
    "    x   = config['x']\n",
    "    hue = config['hue']\n",
    "    row = config['row']\n",
    "\n",
    "    if row != None:\n",
    "        data.sort_values(by=row, inplace=True, kind='mergesort')\n",
    "\n",
    "    g = sns.FacetGrid(\n",
    "        data,\n",
    "        row=row,\n",
    "        col='data_type',\n",
    "        sharex=True,\n",
    "        sharey='row',\n",
    "        height=3,\n",
    "        aspect=2,\n",
    "        margin_titles=True\n",
    "    )\n",
    "    g.map_dataframe(\n",
    "        sns.stripplot,\n",
    "        x=x,\n",
    "        y=target,\n",
    "        hue=hue,\n",
    "        palette='viridis'\n",
    "    )\n",
    "    g.set_axis_labels(x, target)\n",
    "    g.add_legend(title=hue)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tesseract)",
   "language": "python",
   "name": "tesseract"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
